{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and predict data with subset row 1 - 10000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 10001 - 20000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 20001 - 30000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 30001 - 40000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 40001 - 50000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 50001 - 60000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 60001 - 70000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 70001 - 80000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 80001 - 90000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 90001 - 100000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 100001 - 110000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 110001 - 120000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 120001 - 130000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 130001 - 140000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Train and predict data with subset row 140001 - 150000 ... ...\n",
      "Processing data ...\n",
      "Training XGB model ...\n",
      "Writing submission file ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "def Processing(train,test):\n",
    "    \n",
    "    print('Processing data ...')\n",
    "    \n",
    "    # Clean missing values\n",
    "    MissingVal = train[train.columns[1:-1]].isnull().sum().sort_values().apply(lambda x: np.round(x/train.shape[0],2))\n",
    "    Ind = MissingVal[MissingVal>0.5].index\n",
    "    train = train.drop(Ind,axis=1)\n",
    "    test = test.drop(Ind,axis=1)\n",
    "    \n",
    "    # Clean single-valued variables\n",
    "    Features = train[train.columns[1:-1]]\n",
    "    NumVar,NumVarIndex,ObjVar,ObjVarIndex = [],[],[],[]\n",
    "    for x in Features.columns[Features.dtypes!='object']:\n",
    "        NumVar.append(len(Features[x].value_counts()))\n",
    "        NumVarIndex.append(x)\n",
    "    for x in Features.columns[Features.dtypes=='object']:\n",
    "        ObjVar.append(len(Features[x].value_counts()))\n",
    "        ObjVarIndex.append(x)\n",
    "    NumVar = pd.Series(NumVar,index=NumVarIndex).sort_values()\n",
    "    ObjVar = pd.Series(ObjVar,index=ObjVarIndex).sort_values()\n",
    "    Ind = NumVar[:len(NumVar[NumVar==1])].index\n",
    "    train = train.drop(Ind,axis=1)\n",
    "    test = test.drop(Ind,axis=1)\n",
    "    Ind = ObjVar[:len(ObjVar[ObjVar==1])].index\n",
    "    train = train.drop(Ind,axis=1)\n",
    "    test = test.drop(Ind,axis=1)\n",
    "    NumVar = NumVar[len(NumVar[NumVar==1]):]\n",
    "    ObjVar = ObjVar[len(ObjVar[ObjVar==1]):]\n",
    "    \n",
    "    # Filling missing values\n",
    "    MissingValTrain = train[1:-1].isnull().sum().sort_values()\n",
    "    MissingValTest = test[1:-1].isnull().sum().sort_values()\n",
    "    MissingValTrain = MissingValTrain[MissingValTrain>0].index\n",
    "    MissingValTest = MissingValTest[MissingValTest>0].index\n",
    "    for i in MissingValTrain:\n",
    "        Ind = train[i][train[i].isnull()].index\n",
    "        train.loc[Ind,i]=train[i].mode()[0]\n",
    "    for i in MissingValTest:\n",
    "        Ind = test[i][test[i].isnull()].index\n",
    "        test.loc[Ind,i]=test[i].mode()[0]\n",
    "    \n",
    "    # Encoding string variables\n",
    "    \n",
    "    # For variables with only 2 different values, directly apply LabelEncoding inline.\n",
    "    for i in ObjVar[ObjVar==2].index:\n",
    "        le=LabelEncoder()\n",
    "        le.fit(train[i])\n",
    "        train[i]=le.transform(list(train[i]))\n",
    "        test[i]=le.transform(list(test[i]))\n",
    "    \n",
    "    # For variables with >2 but <=5 different values, directly apply OneHotEncoding and drop the original variables.\n",
    "    for i in set(ObjVar[ObjVar<=5].index)-set(ObjVar[ObjVar==2].index):\n",
    "        le = LabelEncoder()\n",
    "        oh = OneHotEncoder()\n",
    "        le.fit(train[i])\n",
    "        train[i] = le.transform(list(train[i]))\n",
    "        test[i] = le.transform(list(test[i]))\n",
    "        temp = np.reshape(list(train[i]),(len(train[i]),-1))\n",
    "        oh.fit(temp)\n",
    "        temp = pd.DataFrame(oh.transform(temp).toarray(),columns=[i+'_%s'%(x) for x in range(ObjVar[i])])\n",
    "        train = pd.concat([train,temp],axis=1).drop(i,axis=1)\n",
    "        temp = np.reshape(list(test[i]),(len(test[i]),-1))\n",
    "        temp = pd.DataFrame(oh.transform(temp).toarray(),columns=[i+'_%s'%(x) for x in range(ObjVar[i])])\n",
    "        test = pd.concat([test,temp],axis=1).drop(i,axis=1)\n",
    "    \n",
    "    # For VAR_0283, VAR_0305 and VAR_0325, some values only contain very few entries (<1% of loadsize), categorize them as \"other\" and apply OneHotEncoding.\n",
    "    # Re-categorize data\n",
    "    for i in set(ObjVar[ObjVar<10].index)-set(ObjVar[ObjVar<=5].index):\n",
    "        valcount = train[i].value_counts()\n",
    "        for j in valcount.index:\n",
    "            if valcount[j]<100:\n",
    "                Ind = train[i][train[i]==j].index\n",
    "                train.loc[Ind,i] = 'Other'\n",
    "        valcount = test[i].value_counts()\n",
    "        for j in valcount.index:\n",
    "            if valcount[j]<100:\n",
    "                Ind = test[i][test[i]==j].index\n",
    "                test.loc[Ind,i] = 'Other'\n",
    "    \n",
    "    # Now apply OneHotEncoding\n",
    "    for i in set(ObjVar[ObjVar<10].index)-set(ObjVar[ObjVar<=5].index):\n",
    "        le = LabelEncoder()\n",
    "        oh = OneHotEncoder()\n",
    "        le.fit(train[i])\n",
    "        train[i] = le.transform(list(train[i]))\n",
    "        test[i] = le.transform(list(test[i]))\n",
    "        temp = np.reshape(list(train[i]),(len(train[i]),-1))\n",
    "        oh.fit(temp)\n",
    "        temp = pd.DataFrame(oh.transform(temp).toarray(),columns=[i+'_%s'%(x) for x in range(len(train[i].value_counts()))])\n",
    "        train = pd.concat([train,temp],axis=1).drop(i,axis=1)\n",
    "        temp = np.reshape(list(test[i]),(len(test[i]),-1))\n",
    "        temp = pd.DataFrame(oh.transform(temp).toarray(),columns=[i+'_%s'%(x) for x in range(len(test[i].value_counts()))])\n",
    "        test = pd.concat([test,temp],axis=1).drop(i,axis=1)\n",
    "        \n",
    "    # VAR_0237, VAR_0342, VAR_0274 and VAR_0200 are state and names. By intuition we convert these string values to the frequency of each entry to represent the popularity of the service in that area.\n",
    "    Features = ['VAR_0237', 'VAR_0342', 'VAR_0274', 'VAR_0200']\n",
    "    for i in Features:\n",
    "        valcount = train[i].value_counts()\n",
    "        for j in valcount.index:\n",
    "            Ind = train[i][train[i] == j].index\n",
    "            train.loc[Ind,i] = valcount[j]\n",
    "        valcount = test[i].value_counts()\n",
    "        for j in valcount.index:\n",
    "            Ind = test[i][test[i] == j].index\n",
    "            test.loc[Ind,i] = valcount[j]\n",
    "            \n",
    "    # VAR_0217, VAR_0204 and VAR_0075 are datetime type variables, convert them to numerical variables.\n",
    "    Features = ['VAR_0217', 'VAR_0204', 'VAR_0075']\n",
    "    Month = {'JAN':'01','FEB':'02','MAR':'03','APR':'04','MAY':'05','JUN':'06','JUL':'07','AUG':'08','SEP':'09','OCT':'10','NOV':'11','DEC':'12'}\n",
    "    for i in Features:\n",
    "        train[i] = train[i].apply(lambda x: int('20'+x[5:7]+Month[x[2:5]]+x[:2]+x[8:10]+x[11:13]+x[14:]))\n",
    "        test[i] = test[i].apply(lambda x: int('20'+x[5:7]+Month[x[2:5]]+x[:2]+x[8:10]+x[11:13]+x[14:]))\n",
    "        \n",
    "    # For VAR_0493 and VAR_0404, there are more than 600 values but most of the them take -1 (>90%), so categorize the rest as \"other\".\n",
    "    Features = ['VAR_0493', 'VAR_0404']\n",
    "    for i in Features:\n",
    "        Ind = train[i][train[i]!='-1'].index\n",
    "        train.loc[Ind,i] = 0\n",
    "        Ind = train[i][train[i]=='-1'].index\n",
    "        train.loc[Ind,i] = -1\n",
    "        Ind = test[i][test[i]!='-1'].index\n",
    "        test.loc[Ind,i] = 0\n",
    "        Ind = test[i][test[i]=='-1'].index\n",
    "        test.loc[Ind,i] = -1\n",
    "        \n",
    "    return train,test\n",
    "\n",
    "\n",
    "def Training(train, test, ID, predict):\n",
    "    \n",
    "    print('Training XGB model ...')\n",
    "    \n",
    "    # Parameters are set based on the gridsearchCV result\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=3, \n",
    "        learning_rate=0.10,\n",
    "        objective='binary:logistic', \n",
    "        eval_metric='auc',\n",
    "        n_estimators=100\n",
    "    )\n",
    "\n",
    "    model.fit(train.drop(['ID','target'],axis=1), train.target)\n",
    "    \n",
    "    # Write to result files\n",
    "    ID = ID.append(test.ID)\n",
    "    prob = model.predict_proba(test.drop('ID',axis=1))\n",
    "    predict = predict.append(pd.DataFrame(prob,columns=['prob1','prob2']).prob2)\n",
    "\n",
    "    return ID, predict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Set up data processing pars\n",
    "    loadsize = 10000\n",
    "    skipsize = 1\n",
    "    ID = pd.Series([])\n",
    "    predict = pd.Series([])\n",
    "    \n",
    "    # Process and train data\n",
    "    while True:\n",
    "        print('Train and predict data with subset row %s - %s ... ...' %(skipsize, skipsize+loadsize-1))\n",
    "        train = pd.read_csv('../Data/train.csv',skiprows=range(1,skipsize),nrows=loadsize,low_memory=False)\n",
    "        test = pd.read_csv('../Data/test.csv',skiprows=range(1,skipsize),nrows=loadsize,low_memory=False)\n",
    "        train, test = Processing(train,test)\n",
    "        ID, predict = Training(train, test, ID, predict)\n",
    "        skipsize += loadsize\n",
    "        if train.shape[0]<loadsize:\n",
    "            break\n",
    "    \n",
    "    # Write the results into the submission file\n",
    "    print('Writing submission file ...')\n",
    "    pd.DataFrame({'ID':ID,'target':predict}).to_csv('submission.csv',index=False)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
